"""
=============================================================================
MONITO PARSER ADAPTER
=============================================================================
–í–µ—Ä—Å–∏—è: 3.0
–¶–µ–ª—å: –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ —Å unified —Å–∏—Å—Ç–µ–º–æ–π
=============================================================================
"""

import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime

from modules.unified_database_manager import UnifiedDatabaseManager
from modules.product_matching_engine import ProductMatchingEngine
from utils.logger import get_logger

logger = get_logger(__name__)

class ParserAdapter:
    """
    –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ —Å unified —Å–∏—Å—Ç–µ–º–æ–π
    """
    
    def __init__(self, db_manager: UnifiedDatabaseManager, 
                 matching_engine: ProductMatchingEngine):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞
        
        Args:
            db_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä unified –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            matching_engine: –î–≤–∏–∂–æ–∫ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤
        """
        self.db_manager = db_manager
        self.matching_engine = matching_engine
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º legacy –ø–∞—Ä—Å–µ—Ä—ã
        self._init_legacy_parsers()
        
        logger.info("Initialized ParserAdapter")
    
    def _init_legacy_parsers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤"""
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä—Å–µ—Ä—ã
            from modules.universal_excel_parser_v2 import UniversalExcelParserV2
            from modules.pdf_parser import PDFParser
            
            self.excel_parser = UniversalExcelParserV2()
            self.pdf_parser = PDFParser()
            
            logger.info("‚úÖ Legacy parsers initialized successfully")
            
        except ImportError as e:
            logger.error(f"‚ùå Failed to import legacy parsers: {e}")
            self.excel_parser = None
            self.pdf_parser = None
    
    # =============================================================================
    # MAIN ADAPTATION METHODS
    # =============================================================================
    
    def process_file_to_unified_system(self, file_path: str, supplier_name: str = None,
                                     auto_match_duplicates: bool = True) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ legacy –ø–∞—Ä—Å–µ—Ä—ã –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ unified —Å–∏—Å—Ç–µ–º—É
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            supplier_name: –ò–º—è –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –±–µ—Ä–µ—Ç—Å—è –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞)
            auto_match_duplicates: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–∫–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        """
        logger.info(f"Processing file to unified system: {file_path}")
        
        start_time = datetime.utcnow()
        result = {
            'file_path': file_path,
            'supplier_name': supplier_name or Path(file_path).stem,
            'processing_start': start_time.isoformat(),
            'legacy_parsing': {},
            'unified_integration': {},
            'matching_results': {},
            'statistics': {}
        }
        
        try:
            # 1. –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ legacy —Å–∏—Å—Ç–µ–º—ã
            logger.info("üìÑ Step 1: Legacy parsing")
            legacy_result = self._parse_with_legacy_parsers(file_path)
            
            if legacy_result.get('error'):
                result['error'] = legacy_result['error']
                return result
            
            result['legacy_parsing'] = legacy_result
            
            # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ unified —Ñ–æ—Ä–º–∞—Ç
            logger.info("üîÑ Step 2: Converting to unified format")
            unified_products = self._convert_to_unified_format(
                legacy_result['products'],
                result['supplier_name']
            )
            
            result['unified_integration']['converted_products'] = len(unified_products)
            
            # 3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ unified database
            logger.info("üíæ Step 3: Integration into unified database")
            integration_stats = self._integrate_into_unified_db(
                unified_products,
                result['supplier_name']
            )
            
            result['unified_integration'].update(integration_stats)
            
            # 4. –ü–æ–∏—Å–∫ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            if auto_match_duplicates:
                logger.info("üîç Step 4: Matching duplicates")
                matching_stats = self._process_duplicate_matching(unified_products)
                result['matching_results'] = matching_stats
            
            # 5. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            end_time = datetime.utcnow()
            processing_time = (end_time - start_time).total_seconds()
            
            result['statistics'] = {
                'processing_time_seconds': processing_time,
                'legacy_products_found': len(legacy_result.get('products', [])),
                'unified_products_created': result['unified_integration'].get('products_created', 0),
                'unified_products_updated': result['unified_integration'].get('products_updated', 0),
                'prices_added': result['unified_integration'].get('prices_added', 0),
                'potential_duplicates_found': result['matching_results'].get('matches_found', 0),
                'processing_end': end_time.isoformat()
            }
            
            logger.info(f"‚úÖ File processing completed in {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error processing file: {e}")
            result['error'] = f"Processing failed: {str(e)}"
            return result
    
    def _parse_with_legacy_parsers(self, file_path: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä—Å–µ—Ä—ã"""
        file_ext = Path(file_path).suffix.lower()
        
        try:
            if file_ext in ['.xlsx', '.xls']:
                if self.excel_parser:
                    logger.debug("Using UniversalExcelParserV2")
                    return self.excel_parser.extract_products_universal(file_path)
                else:
                    return {'error': 'Excel parser not available'}
                    
            elif file_ext == '.pdf':
                if self.pdf_parser:
                    logger.debug("Using PDFParser")
                    return self.pdf_parser.extract_products_from_pdf(file_path)
                else:
                    return {'error': 'PDF parser not available'}
            else:
                return {'error': f'Unsupported file format: {file_ext}'}
                
        except Exception as e:
            logger.error(f"Legacy parsing error: {e}")
            return {'error': f'Legacy parsing failed: {str(e)}'}
    
    def _convert_to_unified_format(self, legacy_products: List[Dict], 
                                 supplier_name: str) -> List[Dict[str, Any]]:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è legacy —Ñ–æ—Ä–º–∞—Ç–∞ —Ç–æ–≤–∞—Ä–æ–≤ –≤ unified —Ñ–æ—Ä–º–∞—Ç
        
        Args:
            legacy_products: –¢–æ–≤–∞—Ä—ã –≤ legacy —Ñ–æ—Ä–º–∞—Ç–µ
            supplier_name: –ò–º—è –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞
            
        Returns:
            –¢–æ–≤–∞—Ä—ã –≤ unified —Ñ–æ—Ä–º–∞—Ç–µ
        """
        unified_products = []
        
        for legacy_product in legacy_products:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ legacy —Ñ–æ—Ä–º–∞—Ç–∞
                original_name = legacy_product.get('original_name', '')
                standardized_name = legacy_product.get('standardized_name', original_name)
                price = legacy_product.get('price', 0)
                unit = legacy_product.get('unit', 'pcs')
                brand = legacy_product.get('brand', '')
                category = legacy_product.get('category', 'general')
                size = legacy_product.get('size', '')
                confidence = legacy_product.get('confidence', 0.8)
                
                # –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
                cleaned_name = self._clean_product_name(standardized_name or original_name)
                if not cleaned_name:
                    continue
                
                cleaned_price = self._clean_price(price)
                if cleaned_price <= 0:
                    continue
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
                size_info = self._extract_size_from_name(cleaned_name)
                if size_info and not size:
                    size = size_info['size']
                    unit = size_info['unit']
                    cleaned_name = size_info['cleaned_name']
                
                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
                if not category or category == 'general':
                    category = self._categorize_product(cleaned_name)
                
                # –°–æ–∑–¥–∞–µ–º unified product
                unified_product = {
                    'standard_name': cleaned_name,
                    'brand': self._clean_brand_name(brand),
                    'category': category,
                    'size': self._parse_size(size) if size else None,
                    'unit': self._normalize_unit(unit),
                    'description': original_name,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
                    'supplier_name': supplier_name,
                    'original_name': original_name,
                    'price': cleaned_price,
                    'confidence_score': confidence
                }
                
                unified_products.append(unified_product)
                
            except Exception as e:
                logger.warning(f"Error converting product {legacy_product}: {e}")
                continue
        
        logger.info(f"Converted {len(unified_products)} products to unified format")
        return unified_products
    
    def _integrate_into_unified_db(self, unified_products: List[Dict], 
                                 supplier_name: str) -> Dict[str, int]:
        """
        –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ç–æ–≤–∞—Ä–æ–≤ –≤ unified database
        
        Args:
            unified_products: –¢–æ–≤–∞—Ä—ã –≤ unified —Ñ–æ—Ä–º–∞—Ç–µ
            supplier_name: –ò–º—è –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        """
        logger.info(f"Integrating {len(unified_products)} products into unified DB")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º bulk import –∏–∑ database manager
        import_stats = self.db_manager.bulk_import_products_and_prices(
            supplier_name, unified_products
        )
        
        logger.info(f"Integration completed: {import_stats}")
        return import_stats
    
    def _process_duplicate_matching(self, unified_products: List[Dict]) -> Dict[str, int]:
        """
        –ü–æ–∏—Å–∫ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
        Args:
            unified_products: –¢–æ–≤–∞—Ä—ã –≤ unified —Ñ–æ—Ä–º–∞—Ç–µ
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        """
        matching_stats = {
            'matches_found': 0,
            'exact_matches': 0,
            'fuzzy_matches': 0,
            'products_processed': 0
        }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ master products –∏–∑ –±–∞–∑—ã
            all_products = self.db_manager.search_master_products("", limit=10000)
            
            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
            for product_data in unified_products:
                try:
                    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π MasterProduct –¥–ª—è –ø–æ–∏—Å–∫–∞
                    from models.unified_database import MasterProduct
                    temp_product = MasterProduct(
                        standard_name=product_data['standard_name'],
                        brand=product_data.get('brand'),
                        category=product_data['category'],
                        size=product_data.get('size'),
                        unit=product_data.get('unit')
                    )
                    
                    # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    matches = self.matching_engine.find_matches(temp_product, limit=5)
                    
                    if matches:
                        matching_stats['matches_found'] += len(matches)
                        
                        for match in matches:
                            if match.match_type.value == 'exact':
                                matching_stats['exact_matches'] += 1
                            else:
                                matching_stats['fuzzy_matches'] += 1
                    
                    matching_stats['products_processed'] += 1
                    
                except Exception as e:
                    logger.warning(f"Error matching product {product_data.get('standard_name')}: {e}")
                    continue
            
            logger.info(f"Duplicate matching completed: {matching_stats}")
            
        except Exception as e:
            logger.error(f"Error in duplicate matching: {e}")
            matching_stats['error'] = str(e)
        
        return matching_stats
    
    # =============================================================================
    # UTILITY METHODS
    # =============================================================================
    
    def _clean_product_name(self, name: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞"""
        if not name:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã
        cleaned = str(name).strip()
        cleaned = ' '.join(cleaned.split())  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        
        # –£–¥–∞–ª—è–µ–º –æ—á–µ–≤–∏–¥–Ω–æ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if len(cleaned) < 2 or cleaned.lower() in ['nan', 'none', 'null', '']:
            return ""
        
        return cleaned
    
    def _clean_price(self, price: Any) -> float:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–Ω—ã"""
        if not price:
            return 0.0
        
        try:
            # –ï—Å–ª–∏ —É–∂–µ —á–∏—Å–ª–æ
            if isinstance(price, (int, float)):
                return float(price)
            
            # –û—á–∏—â–∞–µ–º —Å—Ç—Ä–æ–∫—É
            price_str = str(price).strip()
            
            # –£–¥–∞–ª—è–µ–º –≤–∞–ª—é—Ç–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –±—É–∫–≤—ã
            import re
            price_str = re.sub(r'[^\d.,\-]', '', price_str)
            
            # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É
            price_str = price_str.replace(',', '.')
            
            if not price_str:
                return 0.0
            
            return float(price_str)
            
        except (ValueError, TypeError):
            return 0.0
    
    def _extract_size_from_name(self, name: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞"""
        if not name:
            return None
        
        import re
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–º–µ—Ä–æ–≤
        size_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*(kg|g|ml|l|pcs|pack|box|can|bottle)(?:\s|$)',
            r'(\d+(?:[.,]\d+)?)\s*(–∫–≥|–≥|–º–ª|–ª|—à—Ç|—É–ø–∞–∫|–∫–æ—Ä–æ–±–∫–∞|–±–∞–Ω–∫–∞|–±—É—Ç—ã–ª–∫–∞)(?:\s|$)',
        ]
        
        for pattern in size_patterns:
            match = re.search(pattern, name, re.IGNORECASE)
            if match:
                size_value = match.group(1).replace(',', '.')
                unit = match.group(2).lower()
                
                # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞
                cleaned_name = re.sub(pattern, '', name, flags=re.IGNORECASE).strip()
                cleaned_name = re.sub(r'\s+', ' ', cleaned_name)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
                
                return {
                    'size': float(size_value),
                    'unit': self._normalize_unit(unit),
                    'cleaned_name': cleaned_name
                }
        
        return None
    
    def _categorize_product(self, name: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–æ–≤–∞—Ä–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é"""
        if not name:
            return 'general'
        
        name_lower = name.lower()
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞–ø–∏—Ç–∫–æ–≤
        if any(word in name_lower for word in ['cola', 'sprite', 'fanta', 'pepsi', 'drink', 'juice', 'water', 'beer']):
            return 'beverages'
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ–Ω—Å–µ—Ä–≤–æ–≤
        if any(word in name_lower for word in ['canned', 'tin', 'preserve', '–∫–æ–Ω—Å–µ—Ä–≤', '–±–∞–Ω–∫–∞']):
            return 'canned_food'
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–∞–∫–∞—Ä–æ–Ω
        if any(word in name_lower for word in ['pasta', 'noodle', 'spaghetti', 'macaroni', '–º–∞–∫–∞—Ä–æ–Ω', '–ª–∞–ø—à–∞']):
            return 'pasta_noodles'
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–∞—Å–µ–ª
        if any(word in name_lower for word in ['oil', 'cooking', 'olive', 'sunflower', '–º–∞—Å–ª–æ']):
            return 'cooking_oil'
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–ø–µ—Ü–∏–π
        if any(word in name_lower for word in ['spice', 'seasoning', 'salt', 'pepper', '—Å–ø–µ—Ü–∏–∏', '–ø—Ä–∏–ø—Ä–∞–≤–∞']):
            return 'spices_seasonings'
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–Ω–µ–∫–æ–≤
        if any(word in name_lower for word in ['snack', 'chips', 'cracker', 'biscuit', '—á–∏–ø—Å—ã', '–ø–µ—á–µ–Ω—å–µ']):
            return 'snacks'
        
        return 'general'
    
    def _clean_brand_name(self, brand: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –±—Ä–µ–Ω–¥–∞"""
        if not brand or str(brand).lower() in ['unknown', 'nan', 'none', 'null']:
            return ""
        
        return str(brand).strip()
    
    def _parse_size(self, size: Any) -> Optional[float]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–º–µ—Ä–∞"""
        if not size:
            return None
        
        try:
            if isinstance(size, (int, float)):
                return float(size)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Å—Ç—Ä–æ–∫–∏
            import re
            size_str = str(size).strip()
            match = re.search(r'(\d+(?:[.,]\d+)?)', size_str)
            
            if match:
                return float(match.group(1).replace(',', '.'))
            
        except (ValueError, TypeError):
            pass
        
        return None
    
    def _normalize_unit(self, unit: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è"""
        if not unit:
            return 'pcs'
        
        unit_mapping = {
            '–∫–≥': 'kg', '–≥': 'g', '–º–ª': 'ml', '–ª': 'l',
            '—à—Ç': 'pcs', '—É–ø–∞–∫': 'pack', '–∫–æ—Ä–æ–±–∫–∞': 'box',
            '–±–∞–Ω–∫–∞': 'can', '–±—É—Ç—ã–ª–∫–∞': 'bottle',
            'kilogram': 'kg', 'gram': 'g', 'milliliter': 'ml',
            'liter': 'l', 'piece': 'pcs', 'pieces': 'pcs'
        }
        
        unit_lower = str(unit).lower().strip()
        return unit_mapping.get(unit_lower, unit_lower)
    
    # =============================================================================
    # BATCH PROCESSING
    # =============================================================================
    
    def process_multiple_files(self, file_paths: List[str], 
                             auto_match_duplicates: bool = True) -> Dict[str, Any]:
        """
        –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
        
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
            auto_match_duplicates: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–∫–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
            
        Returns:
            –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        logger.info(f"Starting batch processing of {len(file_paths)} files")
        
        batch_stats = {
            'files_processed': 0,
            'files_successful': 0,
            'files_failed': 0,
            'total_products_found': 0,
            'total_products_integrated': 0,
            'total_duplicates_found': 0,
            'file_results': [],
            'errors': []
        }
        
        for file_path in file_paths:
            try:
                logger.info(f"Processing file: {file_path}")
                
                result = self.process_file_to_unified_system(
                    file_path, 
                    auto_match_duplicates=auto_match_duplicates
                )
                
                batch_stats['files_processed'] += 1
                
                if result.get('error'):
                    batch_stats['files_failed'] += 1
                    batch_stats['errors'].append({
                        'file': file_path,
                        'error': result['error']
                    })
                else:
                    batch_stats['files_successful'] += 1
                    batch_stats['total_products_found'] += result['statistics'].get('legacy_products_found', 0)
                    batch_stats['total_products_integrated'] += result['statistics'].get('unified_products_created', 0)
                    batch_stats['total_duplicates_found'] += result['statistics'].get('potential_duplicates_found', 0)
                
                batch_stats['file_results'].append({
                    'file': file_path,
                    'success': not result.get('error'),
                    'statistics': result.get('statistics', {})
                })
                
            except Exception as e:
                logger.error(f"Error processing file {file_path}: {e}")
                batch_stats['files_failed'] += 1
                batch_stats['errors'].append({
                    'file': file_path,
                    'error': str(e)
                })
        
        logger.info(f"Batch processing completed: {batch_stats['files_successful']}/{batch_stats['files_processed']} successful")
        return batch_stats 