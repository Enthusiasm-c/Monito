#!/usr/bin/env python3
"""
MON-S01: End-to-End Regression Test Suite
–ü–æ–ª–Ω—ã–µ —Ç–µ—Å—Ç—ã —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ pipeline
"""

import os
import sys
import json
import time
import pytest
import tempfile
from pathlib import Path
from typing import List, Dict, Any, Optional
from unittest.mock import patch, MagicMock

# –î–æ–±–∞–≤–ª—è–µ–º path –∫ –º–æ–¥—É–ª—è–º –ø—Ä–æ–µ–∫—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent))

# –û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥—É–ª–∏ –ø—Ä–æ–µ–∫—Ç–∞
try:
    from modules.celery_worker_v2 import CeleryWorkerV2, TaskResult
    from modules.performance_monitor import PerformanceMonitor
    MONITO_MODULES_AVAILABLE = True
except ImportError as e:
    MONITO_MODULES_AVAILABLE = False
    print(f"‚ö†Ô∏è Monito modules –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {e}")

class E2ERegressionSuite:
    """End-to-End —Ç–µ—Å—Ç—ã –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ pipeline"""
    
    def __init__(self):
        self.fixtures_dir = Path("tests/fixtures/evil_files")
        self.expected_dir = Path("tests/fixtures/expected_outputs")
        self.results = []
        self.start_time = time.time()
        
        # Mock —Ä–µ–∂–∏–º –µ—Å–ª–∏ –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
        self.mock_mode = not MONITO_MODULES_AVAILABLE
        
        if self.mock_mode:
            print("üîß –†–∞–±–æ—Ç–∞ –≤ MOCK —Ä–µ–∂–∏–º–µ (–º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã)")
    
    def run_full_regression(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç"""
        
        print("üöÄ –ó–∞–ø—É—Å–∫ MON-S01 End-to-End Regression Suite")
        print("=" * 60)
        
        test_results = {
            'suite_name': 'MON-S01 E2E Regression',
            'start_time': self.start_time,
            'mock_mode': self.mock_mode,
            'tests': [],
            'summary': {}
        }
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ fixtures
        fixtures_result = self.test_fixtures_availability()
        test_results['tests'].append(fixtures_result)
        
        # 2. –¢–µ—Å—Ç –∫–∞–∂–¥–æ–≥–æ evil fixture
        evil_fixtures = self.get_evil_fixtures()
        for fixture_info in evil_fixtures:
            result = self.test_single_fixture(fixture_info)
            test_results['tests'].append(result)
        
        # 3. –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤)
        batch_result = self.test_batch_processing()
        test_results['tests'].append(batch_result)
        
        # 4. –°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        perf_result = self.test_performance_regression()
        test_results['tests'].append(perf_result)
        
        # 5. –¢–µ—Å—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –æ—à–∏–±–∫–∞–º
        error_result = self.test_error_handling()
        test_results['tests'].append(error_result)
        
        # –°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        test_results['summary'] = self.generate_summary(test_results['tests'])
        test_results['end_time'] = time.time()
        test_results['total_duration'] = test_results['end_time'] - test_results['start_time']
        
        self.save_test_report(test_results)
        self.print_summary(test_results)
        
        return test_results
    
    def test_fixtures_availability(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ fixtures"""
        
        print("\nüìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Evil Fixtures...")
        
        result = {
            'test_name': 'fixtures_availability',
            'status': 'UNKNOWN',
            'details': {},
            'duration': 0
        }
        
        start = time.time()
        
        try:
            manifest_file = self.fixtures_dir / "fixtures_manifest.json"
            
            if not manifest_file.exists():
                result['status'] = 'FAILED'
                result['details']['error'] = '–ú–∞–Ω–∏—Ñ–µ—Å—Ç fixtures –Ω–µ –Ω–∞–π–¥–µ–Ω'
                return result
            
            with open(manifest_file, 'r', encoding='utf-8') as f:
                manifest = json.load(f)
            
            fixtures = manifest['mon_s01_fixtures']['fixtures']
            available_count = 0
            missing_files = []
            
            for fixture in fixtures:
                file_path = self.fixtures_dir / fixture['filename']
                if file_path.exists():
                    available_count += 1
                    print(f"  ‚úÖ {fixture['filename']}")
                else:
                    missing_files.append(fixture['filename'])
                    print(f"  ‚ùå {fixture['filename']} - –û–¢–°–£–¢–°–¢–í–£–ï–¢")
            
            result['details'] = {
                'total_fixtures': len(fixtures),
                'available_fixtures': available_count,
                'missing_files': missing_files,
                'manifest': manifest
            }
            
            if missing_files:
                result['status'] = 'PARTIAL'
            else:
                result['status'] = 'PASSED'
                
        except Exception as e:
            result['status'] = 'FAILED'
            result['details']['error'] = str(e)
        
        result['duration'] = time.time() - start
        return result
    
    def test_single_fixture(self, fixture_info: Dict[str, Any]) -> Dict[str, Any]:
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ evil fixture"""
        
        filename = fixture_info['filename']
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {filename}...")
        
        result = {
            'test_name': f'single_fixture_{filename}',
            'status': 'UNKNOWN',
            'details': {},
            'duration': 0
        }
        
        start = time.time()
        
        try:
            file_path = self.fixtures_dir / filename
            
            if not file_path.exists():
                result['status'] = 'SKIPPED'
                result['details']['error'] = f'–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω'
                return result
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            expected_file = self.expected_dir / f"{filename.replace('.', '_').replace('txt', 'json')}_expected.json"
            if expected_file.exists():
                with open(expected_file, 'r', encoding='utf-8') as f:
                    expected = json.load(f)
            else:
                expected = fixture_info
            
            if self.mock_mode:
                # Mock –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞
                mock_result = self.mock_file_processing(file_path, expected)
                result['details'] = mock_result
                result['status'] = 'PASSED' if mock_result['success'] else 'FAILED'
            else:
                # –†–µ–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞
                processing_result = self.real_file_processing(file_path, expected)
                result['details'] = processing_result
                result['status'] = 'PASSED' if processing_result['success'] else 'FAILED'
                
        except Exception as e:
            result['status'] = 'FAILED'
            result['details']['error'] = str(e)
        
        result['duration'] = time.time() - start
        return result
    
    def mock_file_processing(self, file_path: Path, expected: Dict[str, Any]) -> Dict[str, Any]:
        """Mock –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–æ–¥—É–ª–µ–π"""
        
        file_size = file_path.stat().st_size
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
        processing_time = min(file_size / 100000, 5.0)  # –ò–º–∏—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        time.sleep(processing_time * 0.1)  # –°–æ–∫—Ä–∞—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–∂–∏–¥–∞–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        rows_extracted = expected.get('expected_rows_in', 0)
        rows_processed = expected.get('expected_rows_out', rows_extracted)
        
        # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–π–ª—ã –º–æ–≥—É—Ç "–ø—Ä–æ–≤–∞–ª–∏—Ç—å—Å—è" –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
        challenges = expected.get('challenges', [])
        difficulty_score = len(challenges)
        
        # –°–ª–æ–∂–Ω—ã–µ —Ñ–∞–π–ª—ã –∏–º–µ—é—Ç –±–æ–ª—å—à–µ —à–∞–Ω—Å–æ–≤ –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã
        success_rate = max(0.7, 1.0 - (difficulty_score * 0.1))
        success = (difficulty_score < 5)  # –ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–∞–≤–∏–ª–æ –¥–ª—è mock
        
        result = {
            'success': success,
            'file_size_bytes': file_size,
            'file_size_mb': round(file_size / (1024*1024), 2),
            'processing_time': processing_time,
            'rows_extracted': rows_extracted,
            'rows_processed': rows_processed,
            'challenges_detected': challenges,
            'difficulty_score': difficulty_score,
            'mock_simulation': True
        }
        
        if not success:
            result['error'] = f"Mock failure: –≤—ã—Å–æ–∫–∏–π difficulty_score ({difficulty_score})"
        
        return result
    
    def real_file_processing(self, file_path: Path, expected: Dict[str, Any]) -> Dict[str, Any]:
        """–†–µ–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ Celery Worker"""
        
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º worker –≤ mock —Ä–µ–∂–∏–º–µ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
            worker = CeleryWorkerV2(mock_mode=True)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
            if file_path.suffix.lower() in ['.csv']:
                task_result = worker.process_csv_file(str(file_path))
            elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                task_result = worker.process_excel_file(str(file_path))
            elif file_path.suffix.lower() in ['.txt']:
                # –î–ª—è mock PDF/OCR —Ñ–∞–π–ª–æ–≤
                task_result = worker.process_text_file(str(file_path))
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_path.suffix}")
            
            result = {
                'success': task_result.status == 'completed',
                'task_id': task_result.task_id,
                'processing_time': task_result.processing_time,
                'rows_extracted': getattr(task_result, 'rows_extracted', 0),
                'rows_processed': getattr(task_result, 'rows_processed', 0),
                'file_size_bytes': file_path.stat().st_size,
                'worker_mode': 'real'
            }
            
            if task_result.status != 'completed':
                result['error'] = task_result.error_message
            
            return result
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'worker_mode': 'real_failed'
            }
    
    def test_batch_processing(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ"""
        
        print(f"\nüì¶ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ batch –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
        
        result = {
            'test_name': 'batch_processing',
            'status': 'UNKNOWN',
            'details': {},
            'duration': 0
        }
        
        start = time.time()
        
        try:
            # –í—ã–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∏
            available_files = list(self.fixtures_dir.glob("*.csv"))[:3]  # –ü–µ—Ä–≤—ã–µ 3 CSV
            
            if len(available_files) < 2:
                result['status'] = 'SKIPPED'
                result['details']['error'] = '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è batch —Ç–µ—Å—Ç–∞'
                return result
            
            if self.mock_mode:
                # Mock batch –æ–±—Ä–∞–±–æ—Ç–∫–∞
                batch_results = []
                total_time = 0
                
                for file_path in available_files:
                    mock_result = self.mock_file_processing(file_path, {})
                    batch_results.append({
                        'filename': file_path.name,
                        'success': mock_result['success'],
                        'processing_time': mock_result['processing_time']
                    })
                    total_time += mock_result['processing_time']
                
                result['details'] = {
                    'files_processed': len(available_files),
                    'total_time': total_time,
                    'average_time': total_time / len(available_files),
                    'results': batch_results,
                    'mode': 'mock'
                }
                
                success_count = sum(1 for r in batch_results if r['success'])
                result['status'] = 'PASSED' if success_count >= len(available_files) * 0.7 else 'PARTIAL'
            
            else:
                # –†–µ–∞–ª—å–Ω–∞—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Celery
                worker = CeleryWorkerV2(mock_mode=True)
                
                batch_results = []
                for file_path in available_files:
                    task_result = worker.process_csv_file(str(file_path))
                    batch_results.append({
                        'filename': file_path.name,
                        'task_id': task_result.task_id,
                        'success': task_result.status == 'completed',
                        'processing_time': task_result.processing_time
                    })
                
                result['details'] = {
                    'files_processed': len(available_files),
                    'results': batch_results,
                    'mode': 'real'
                }
                
                success_count = sum(1 for r in batch_results if r['success'])
                result['status'] = 'PASSED' if success_count >= len(available_files) * 0.7 else 'PARTIAL'
                
        except Exception as e:
            result['status'] = 'FAILED'
            result['details']['error'] = str(e)
        
        result['duration'] = time.time() - start
        return result
    
    def test_performance_regression(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–∏–π"""
        
        print(f"\n‚ö° –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        
        result = {
            'test_name': 'performance_regression',
            'status': 'UNKNOWN',
            'details': {},
            'duration': 0
        }
        
        start = time.time()
        
        try:
            # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª –¥–ª—è —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∞
            large_file = self.fixtures_dir / "large_data.csv"
            
            if not large_file.exists():
                result['status'] = 'SKIPPED'
                result['details']['error'] = '–ë–æ–ª—å—à–æ–π —Ñ–∞–π–ª –¥–ª—è —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω'
                return result
            
            file_size_mb = large_file.stat().st_size / (1024*1024)
            
            # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (MON-007 targets)
            target_throughput_mb_per_sec = 10.0  # 10 MB/sec –º–∏–Ω–∏–º—É–º
            target_max_time_sec = file_size_mb / target_throughput_mb_per_sec
            
            if self.mock_mode:
                # Mock –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                mock_processing_time = file_size_mb / 15.0  # –°–∏–º—É–ª—è—Ü–∏—è 15 MB/sec
                
                result['details'] = {
                    'file_size_mb': round(file_size_mb, 2),
                    'processing_time_sec': round(mock_processing_time, 2),
                    'throughput_mb_per_sec': round(file_size_mb / mock_processing_time, 2),
                    'target_throughput': target_throughput_mb_per_sec,
                    'target_max_time': round(target_max_time_sec, 2),
                    'mode': 'mock'
                }
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ü–µ–ª—è–º
                actual_throughput = file_size_mb / mock_processing_time
                if actual_throughput >= target_throughput_mb_per_sec:
                    result['status'] = 'PASSED'
                else:
                    result['status'] = 'FAILED'
                    result['details']['performance_issue'] = '–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∏–∂–µ —Ü–µ–ª–µ–≤–æ–π'
            
            else:
                # –†–µ–∞–ª—å–Ω—ã–π performance —Ç–µ—Å—Ç
                monitor = PerformanceMonitor()
                monitor.start_monitoring('e2e_performance_test')
                
                worker = CeleryWorkerV2(mock_mode=True)
                task_result = worker.process_csv_file(str(large_file))
                
                perf_stats = monitor.stop_monitoring('e2e_performance_test')
                
                result['details'] = {
                    'file_size_mb': round(file_size_mb, 2),
                    'processing_time_sec': task_result.processing_time,
                    'throughput_mb_per_sec': round(file_size_mb / task_result.processing_time, 2),
                    'target_throughput': target_throughput_mb_per_sec,
                    'performance_stats': perf_stats,
                    'mode': 'real'
                }
                
                actual_throughput = file_size_mb / task_result.processing_time
                if actual_throughput >= target_throughput_mb_per_sec and task_result.status == 'completed':
                    result['status'] = 'PASSED'
                else:
                    result['status'] = 'FAILED'
                    result['details']['performance_issue'] = '–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–ª–∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –Ω–∏–∂–µ —Ü–µ–ª–µ–≤–æ–π'
                
        except Exception as e:
            result['status'] = 'FAILED'
            result['details']['error'] = str(e)
        
        result['duration'] = time.time() - start
        return result
    
    def test_error_handling(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –æ—à–∏–±–∫–∞–º"""
        
        print(f"\nüõ°Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –æ—à–∏–±–∫–∞–º...")
        
        result = {
            'test_name': 'error_handling',
            'status': 'UNKNOWN',
            'details': {},
            'duration': 0
        }
        
        start = time.time()
        
        try:
            error_scenarios = [
                ('nonexistent_file.csv', '–ù–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª'),
                ('', '–ü—É—Å—Ç–æ–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É'),
                ('/invalid/path/file.csv', '–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø—É—Ç—å'),
            ]
            
            error_results = []
            
            for invalid_path, scenario_name in error_scenarios:
                try:
                    if self.mock_mode:
                        # Mock –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
                        error_result = {
                            'scenario': scenario_name,
                            'input': invalid_path,
                            'handled_gracefully': True,
                            'error_type': 'mock_file_not_found',
                            'mode': 'mock'
                        }
                    else:
                        # –†–µ–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
                        worker = CeleryWorkerV2(mock_mode=True)
                        task_result = worker.process_csv_file(invalid_path)
                        
                        error_result = {
                            'scenario': scenario_name,
                            'input': invalid_path,
                            'handled_gracefully': task_result.status == 'failed',
                            'error_message': task_result.error_message,
                            'mode': 'real'
                        }
                    
                    error_results.append(error_result)
                    
                except Exception as e:
                    error_results.append({
                        'scenario': scenario_name,
                        'input': invalid_path,
                        'handled_gracefully': False,
                        'unhandled_exception': str(e)
                    })
            
            result['details'] = {
                'scenarios_tested': len(error_scenarios),
                'error_results': error_results
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –æ—à–∏–±–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
            graceful_count = sum(1 for r in error_results if r.get('handled_gracefully', False))
            if graceful_count == len(error_scenarios):
                result['status'] = 'PASSED'
            else:
                result['status'] = 'PARTIAL'
                result['details']['issue'] = f'–¢–æ–ª—å–∫–æ {graceful_count}/{len(error_scenarios)} –æ—à–∏–±–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ'
                
        except Exception as e:
            result['status'] = 'FAILED'
            result['details']['error'] = str(e)
        
        result['duration'] = time.time() - start
        return result
    
    def get_evil_fixtures(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ evil fixtures –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        
        try:
            manifest_file = self.fixtures_dir / "fixtures_manifest.json"
            with open(manifest_file, 'r', encoding='utf-8') as f:
                manifest = json.load(f)
            return manifest['mon_s01_fixtures']['fixtures']
        except:
            # Fallback –Ω–∞ –ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –Ω–∞–ø—Ä—è–º—É—é
            return [
                {'filename': f.name, 'challenges': ['unknown']}
                for f in self.fixtures_dir.glob("*.csv")
            ]
    
    def generate_summary(self, test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–æ–≤"""
        
        total_tests = len(test_results)
        passed = sum(1 for r in test_results if r['status'] == 'PASSED')
        failed = sum(1 for r in test_results if r['status'] == 'FAILED')
        partial = sum(1 for r in test_results if r['status'] == 'PARTIAL')
        skipped = sum(1 for r in test_results if r['status'] == 'SKIPPED')
        
        total_duration = sum(r.get('duration', 0) for r in test_results)
        
        return {
            'total_tests': total_tests,
            'passed': passed,
            'failed': failed,
            'partial': partial,
            'skipped': skipped,
            'pass_rate': round((passed / total_tests) * 100, 1) if total_tests > 0 else 0,
            'total_duration_sec': round(total_duration, 2),
            'average_test_duration': round(total_duration / total_tests, 2) if total_tests > 0 else 0
        }
    
    def save_test_report(self, test_results: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –æ —Ç–µ—Å—Ç–∞—Ö"""
        
        reports_dir = Path("tests/reports")
        reports_dir.mkdir(exist_ok=True)
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        report_file = reports_dir / f"mon_s01_e2e_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
    
    def print_summary(self, test_results: Dict[str, Any]):
        """–ü–µ—á–∞—Ç–∞–µ—Ç —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        
        summary = test_results['summary']
        
        print("\n" + "="*60)
        print("üìä –°–í–û–î–ö–ê MON-S01 E2E REGRESSION SUITE")
        print("="*60)
        
        print(f"üß™ –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {summary['total_tests']}")
        print(f"‚úÖ –ü—Ä–æ–π–¥–µ–Ω–æ: {summary['passed']}")
        print(f"‚ùå –ü—Ä–æ–≤–∞–ª–µ–Ω–æ: {summary['failed']}")
        print(f"‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ: {summary['partial']}")
        print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: {summary['skipped']}")
        print(f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {summary['pass_rate']}%")
        print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {summary['total_duration_sec']} —Å–µ–∫")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å
        if summary['failed'] == 0 and summary['partial'] == 0:
            overall_status = "üéØ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´"
        elif summary['failed'] == 0:
            overall_status = "‚ö†Ô∏è –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –° –ó–ê–ú–ï–ß–ê–ù–ò–Ø–ú–ò"
        else:
            overall_status = "‚ùå –ï–°–¢–¨ –ü–†–û–í–ê–õ–ï–ù–ù–´–ï –¢–ï–°–¢–´"
        
        print(f"\nüèÅ –û–±—â–∏–π —Å—Ç–∞—Ç—É—Å: {overall_status}")
        
        if test_results.get('mock_mode'):
            print("\nüîß –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¢–µ—Å—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –≤ MOCK —Ä–µ–∂–∏–º–µ")
        
        print("="*60)

# Pytest –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
class TestMONS01E2ERegressionSuite:
    """Pytest –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ E2E —Ç–µ—Å—Ç–æ–≤"""
    
    @pytest.fixture(scope="class")
    def e2e_suite(self):
        """–§–∏–∫—Å—Ç—É—Ä–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è E2E test suite"""
        return E2ERegressionSuite()
    
    def test_e2e_regression_full(self, e2e_suite):
        """–û—Å–Ω–æ–≤–Ω–æ–π E2E —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç"""
        
        results = e2e_suite.run_full_regression()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏
        summary = results['summary']
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–π —É—Å–ø–µ—Ö–∞: –º–∏–Ω–∏–º—É–º 70% —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ, –Ω–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≤–∞–ª–æ–≤
        success_rate = summary['pass_rate']
        critical_failures = summary['failed']
        
        assert success_rate >= 70.0, f"–ù–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞ E2E —Ç–µ—Å—Ç–æ–≤: {success_rate}%"
        assert critical_failures <= 1, f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≤–∞–ª–æ–≤: {critical_failures}"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        pytest.current_test_results = results

def main():
    """–ó–∞–ø—É—Å–∫ E2E —Ç–µ—Å—Ç–æ–≤ –Ω–∞–ø—Ä—è–º—É—é"""
    
    suite = E2ERegressionSuite()
    results = suite.run_full_regression()
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–¥ –æ—à–∏–±–∫–∏ –¥–ª—è CI
    summary = results['summary']
    if summary['failed'] > 0:
        return 1  # –ï—Å—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–∞–ª—ã
    elif summary['pass_rate'] < 70:
        return 2  # –ù–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞
    else:
        return 0  # –í—Å–µ —Ö–æ—Ä–æ—à–æ

if __name__ == "__main__":
    sys.exit(main()) 